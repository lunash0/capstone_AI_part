import os
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration

# Load Model and Tokenizer
tokenizer = PreTrainedTokenizerFast.from_pretrained("EbanLee/kobart-summary-v3")
model = BartForConditionalGeneration.from_pretrained("EbanLee/kobart-summary-v3")

# Encoding
cur_dir = os.getcwd()
input_file = os.path.join(cur_dir, 'input.txt')
with open(input_file, "r", encoding="utf-8") as file:
    input_text = file.read()
# input_text = "10? ?? ?? ????? ??? ?? ??? ??? ? '??? ??'? ???? ???? ?? ???? ????? ??? ??? ???? ??? ?? '????'?? ???.\n ??? ????? ??? 8? ?? ??? ???? ?? ??? ????? ??? ?? ????? ?? ????? ???? ?? ?? ????? ????? ????.\n '??? ?? ?????? ????'(???? ???,???, ?? '?????')? ??? ?? ?????? ???? ? 224?.\n ? ?? 2014~2016? ??? ?????? ?? ??? ??? ??? '????'? ??? ??? ?????????? ??, ??? ? ?????(????? ??), ??? ??? ?? ??.\n ????? ??? ??? '???' '??' ?? ?? ??? ??? ?????(??) ???.\n '????'? 14? ?? ?? ???? ? ???? ?????? ?? ? ?? ??? ?? ???? ?? ???? ?? ??? ?? ??(1/2 ?? ???)? ??? ????.\n ? ????? 13? ??? ???? '? ?? ?? ???? ??? ???? ??? ????'?? '??? 8? ?? ????? ?? ??? ???? ???? ?? ????? ???? ??'? ???.\n ?? ????? ???? ???? ??? ???? ? ?? ?????? ????? ????.\n ??? ??? ????? ^???? ??? ??? ???? ???? ??? ???? ???? ??? ?? ?? ^???? 21??? ???? ??? ???? ??? ????? ??? ??? ???? ^????? ??? ??? ?? ??? ?????? ???? ??? ???.\n ?? ??? ??? ??? '??? ???? ??? ? ?? ?????? ???? ? ??? ??? ??? ??? ???? ????? ?? ??? ????? ? ??? ?'???? ?? ?????? ???? ?????.\n '????'? ??? ??? ??? ????? ??? '??? ? ???? ??? ??? ??.\n ?? ??? ?? ??, ???? ??? ??? ????, ???? ????? ?? ????? ??? ? ???'? ???.\n ???? '??? ????? ???? ??? ??? ???? ? ?? ??? '??'?? ? ??? ?? ?? ??? ?????'? ????.\n"
inputs = tokenizer(input_text, return_tensors="pt", padding="max_length", truncation=True, max_length=1026)

# Generate Summary Text Ids
summary_text_ids = model.generate(
input_ids=inputs['input_ids'],
attention_mask=inputs['attention_mask'],
bos_token_id=model.config.bos_token_id,
eos_token_id=model.config.eos_token_id,
length_penalty=1.0,
max_length=1000,
min_length=100,
num_beams=10,
repetition_penalty=1.5,
no_repeat_ngram_size=15,
)

# Decoding Text Ids
print(tokenizer.decode(summary_text_ids[0], skip_special_tokens=True))

# https://huggingface.co/EbanLee/kobart-summary-v3/blob/main/model.safetensors
